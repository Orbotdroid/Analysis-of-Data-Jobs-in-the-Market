{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: splinter in /anaconda3/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: selenium>=3.141.0 in /anaconda3/lib/python3.7/site-packages (from splinter) (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /anaconda3/lib/python3.7/site-packages (from selenium>=3.141.0->splinter) (1.23)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install splinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup_splinter = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_job_pages = []\n",
    "def li_scraper(keyword, location, pages):\n",
    "#Converting all keyword strings with spaces into url format with \"%20\"\n",
    "    url_keyword = []\n",
    "    for i in range(0, len(keyword)):\n",
    "        if keyword[i] == \" \":\n",
    "            url_keyword.append(\"%20\")\n",
    "        else: url_keyword.append(keyword[i])\n",
    "        key_string = ''.join(url_keyword)\n",
    "    \n",
    "#Same for location string\n",
    "    url_location = []\n",
    "    for i in range(0, len(location)):\n",
    "        if location[i] == \" \":\n",
    "            url_location.append(\"%20\")\n",
    "        else: url_location.append(location[i])\n",
    "        loc_string = ''.join(url_location)\n",
    "#Generating url\n",
    "    for i in range(0,int(pages)):\n",
    "        url = f\"https://www.linkedin.com/jobs/search/?keywords={key_string}&location={loc_string}%2C%20California&locationId=PLACES.us.7-1-0-38-1&start=\" + str(i*25)\n",
    "#Scraping url\n",
    "        browser.visit(url)\n",
    "        html = browser.html\n",
    "        soup_splinter = bs(html, 'html.parser')\n",
    "        li_job_pages.append(soup_splinter)\n",
    "#These variables store their associated category of data, tags included\n",
    "    job_titles = []\n",
    "    for i in range(0, len(li_job_pages)):\n",
    "        job_titles.append(li_job_pages[i].find_all(\"h3\", attrs={\"class\": \"listed-job-posting__title\",\n",
    "                                   }))\n",
    "    job_locations = []\n",
    "    for i in range(0, len(li_job_pages)):\n",
    "        job_locations.append(li_job_pages[i].find_all(\"p\", attrs={\"class\":\"listed-job-posting__location\"}))\n",
    "    job_companies = []\n",
    "    for i in range(0, len(li_job_pages)):\n",
    "        job_companies.append(li_job_pages[i].find_all(\"h4\", attrs={\"class\": \"listed-job-posting__company\"}))\n",
    "    job_time_posted_ago = []\n",
    "    for i in range(0, len(li_job_pages)):\n",
    "        job_time_posted_ago.append(li_job_pages[i].find_all(\"span\", attrs={\"class\":\"listed-job-posting__flavor posted-time-ago__text\"}))\n",
    "#These variables contain the same data with tags removed.\n",
    "    locations = []\n",
    "    for i in range(0, len(job_locations)):\n",
    "        for j in range(0, len(job_locations[i])):\n",
    "            locations.append(job_locations[i][j].get_text())\n",
    "    time_posted = []\n",
    "    for i in range(0, len(job_time_posted_ago)):\n",
    "        for j in range(0, len(job_time_posted_ago[i])):\n",
    "            time_posted.append(job_time_posted_ago[i][j].get_text())\n",
    "    companies = []\n",
    "    for i in range(0, len(job_companies)):\n",
    "        for j in range(0, len(job_time_posted_ago[i])):\n",
    "            companies.append(job_companies[i][j].get_text())\n",
    "    titles = []\n",
    "    for i in range(0, len(job_titles)):\n",
    "        for j in range(0, len(job_titles[i])):\n",
    "            titles.append(job_titles[i][j].get_text())\n",
    "#Return locations, time posted, compaines and job titles\n",
    "    return time_posted, companies, titles, locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['3 hours ago',\n",
       "  '19 hours ago',\n",
       "  '15 hours ago',\n",
       "  '6 hours ago',\n",
       "  '13 hours ago',\n",
       "  '15 hours ago',\n",
       "  '12 hours ago',\n",
       "  '16 hours ago',\n",
       "  '1 hour ago',\n",
       "  '4 days ago',\n",
       "  '3 days ago',\n",
       "  '1 week ago',\n",
       "  '4 days ago',\n",
       "  '7 days ago',\n",
       "  '6 days ago',\n",
       "  '15 hours ago',\n",
       "  '1 month ago',\n",
       "  '3 weeks ago',\n",
       "  '4 weeks ago',\n",
       "  '1 month ago',\n",
       "  '2 weeks ago',\n",
       "  '1 month ago',\n",
       "  '2 days ago',\n",
       "  '1 month ago',\n",
       "  '2 weeks ago',\n",
       "  '14 hours ago',\n",
       "  '1 day ago',\n",
       "  '4 weeks ago',\n",
       "  '4 months ago',\n",
       "  '3 hours ago',\n",
       "  '2 months ago',\n",
       "  '2 months ago',\n",
       "  '2 months ago',\n",
       "  '2 months ago',\n",
       "  '2 weeks ago',\n",
       "  '2 days ago',\n",
       "  '2 months ago',\n",
       "  '2 months ago',\n",
       "  '5 months ago',\n",
       "  '5 days ago',\n",
       "  '2 months ago',\n",
       "  '4 days ago',\n",
       "  '22 hours ago',\n",
       "  '3 days ago',\n",
       "  '14 hours ago',\n",
       "  '2 days ago',\n",
       "  '4 months ago',\n",
       "  '1 week ago',\n",
       "  '2 days ago',\n",
       "  '5 days ago',\n",
       "  '5 days ago',\n",
       "  '2 days ago',\n",
       "  '1 month ago',\n",
       "  '1 month ago',\n",
       "  '4 months ago',\n",
       "  '1 year ago',\n",
       "  '11 hours ago',\n",
       "  '2 days ago',\n",
       "  '3 hours ago',\n",
       "  '18 hours ago',\n",
       "  '4 days ago',\n",
       "  '3 months ago',\n",
       "  '2 months ago',\n",
       "  '4 days ago',\n",
       "  '3 months ago',\n",
       "  '1 week ago',\n",
       "  '2 months ago',\n",
       "  '1 day ago',\n",
       "  '3 months ago',\n",
       "  '9 hours ago',\n",
       "  '2 weeks ago',\n",
       "  '4 weeks ago',\n",
       "  '4 weeks ago',\n",
       "  '1 month ago',\n",
       "  '1 month ago'],\n",
       " ['Seasoned',\n",
       "  'Faire',\n",
       "  'Gusto',\n",
       "  'Diligente Technologies',\n",
       "  'Airbnb',\n",
       "  'Five9',\n",
       "  'Intrexon Corporation',\n",
       "  'Western Union',\n",
       "  'Bey',\n",
       "  'Personal Capital',\n",
       "  'Turo',\n",
       "  'Wish',\n",
       "  'Collabera Inc.',\n",
       "  'HONOR NYC',\n",
       "  \"O'Reilly Media\",\n",
       "  'Databricks',\n",
       "  'VIZIO',\n",
       "  'APN Consulting Inc.',\n",
       "  '360pi (acquired by Market Track)',\n",
       "  'Allianz Global Investors',\n",
       "  'Youper',\n",
       "  'Omada Health',\n",
       "  'Pinterest',\n",
       "  'Tubi',\n",
       "  'Blueshift',\n",
       "  'CyberCoders',\n",
       "  'Zignal Labs',\n",
       "  'eero',\n",
       "  'Sony Playstation Network',\n",
       "  'Skyrocket Ventures',\n",
       "  'Ridecell',\n",
       "  'Cambly Inc.',\n",
       "  'Scale AI',\n",
       "  'SHIFT.com',\n",
       "  'The Dotcom Team',\n",
       "  'Optimizely',\n",
       "  'University of California, San Francisco',\n",
       "  'Plaid',\n",
       "  'RiskIQ',\n",
       "  'Nisum',\n",
       "  'Zeus ',\n",
       "  'Trianz',\n",
       "  'Smith Hanley Associates',\n",
       "  'One Medical',\n",
       "  'Sunrun',\n",
       "  'AppDynamics',\n",
       "  'Sony Corporation of America',\n",
       "  'InfoSmart Technologies,Inc.',\n",
       "  'BlackThorn Therapeutics',\n",
       "  'Yelp',\n",
       "  'Convergent Genomics ',\n",
       "  'Earnest Inc.',\n",
       "  'Siren',\n",
       "  'Asana',\n",
       "  'Square',\n",
       "  'Kanjoya, Inc',\n",
       "  'Unity Technologies',\n",
       "  'Eventbrite',\n",
       "  'PubMatic',\n",
       "  'Andiamo Partners',\n",
       "  'DoorDash',\n",
       "  'Intelletec',\n",
       "  'Udemy',\n",
       "  'LiveRamp',\n",
       "  'Lime',\n",
       "  'PADI',\n",
       "  'CircleUp',\n",
       "  'eFinancialCareers',\n",
       "  'Invitae',\n",
       "  'Amobee',\n",
       "  'Cynet Systems Inc',\n",
       "  'Numerator',\n",
       "  'Rally Health',\n",
       "  'Jyve Corporation',\n",
       "  'EDO'],\n",
       " ['Data Analyst',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist, Product Analytics',\n",
       "  'Lead Big Data Engineer',\n",
       "  'Data Scientist - Foundation',\n",
       "  'Senior Software Engineer - Big Data',\n",
       "  'Data Scientist/Engineer',\n",
       "  'Data Scientist',\n",
       "  '(Permanent Jobs) Principal Data Scientist with PHD & Data Scientist (Product Analytics) - San Francisco',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist, Machine Learning Practice',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist, Experimentation (Statistics)',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist - Machine Learning, Hadoop, Data Mining',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Sr. Machine Learning Engineer / Head of Data Science (A.I. Startup)',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist - Statistical Inference',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Principal Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist, Tariff and Policy',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist, Neuroimaging',\n",
       "  'Applied Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist, Growth',\n",
       "  'Data Scientist, Operations',\n",
       "  'Data Scientist',\n",
       "  'Senior Data Scientist',\n",
       "  'Data Scientist - Risk & Detection',\n",
       "  'Senior Data Scientist / Machine Learning Engineer',\n",
       "  'Staff Data Scientist - San Ramon, CA',\n",
       "  'Data Scientist, Machine Learning - Fraud & Risk',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist - Data Products',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist - PADI',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Scientist/Machine Learning Engineer, Applied Science',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist Intern',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist'],\n",
       " ['San Francisco, California',\n",
       "  'San Francisco, California, United States',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco Bay Area',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Ramon, California',\n",
       "  'South San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'Oakland, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco Bay Area',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco Bay Area',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'Oakland, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco Bay Area',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, California',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'South San Francisco, CA, US',\n",
       "  'San Francisco, California, United States',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco Bay Area',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'Redwood City, CA, US',\n",
       "  'San Ramon, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'Redwood City, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US',\n",
       "  'San Francisco, CA, US'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_scraper(\"data scientist\", \"san francisco\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
